{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1733a166",
   "metadata": {},
   "source": [
    "## Test of different regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c138f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from tabpfn import TabPFNRegressor  \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71dbbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please specify the path to the root directory of the repository\n",
    "ROOT_PATH = '/Users/wolffjoa/data_local/hicgan/hyperparameter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9cfe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(os.path.join(ROOT_PATH, 'merged_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b70eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d91f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "features = ['pearson_AUC', 'hicrep', 'TAD_fraction', 'TAD_fraction_exact_match', 'TAD_score_MSE']\n",
    "feature_mapping = {\n",
    "    'pearson_AUC': 'Pearson AUC',\n",
    "    'hicrep': 'HiCRep',\n",
    "    'TAD_fraction': 'TAD fraction',\n",
    "    'TAD_fraction_exact_match': 'TAD FEM',\n",
    "    'TAD_score_MSE': 'TAD score MSE'\n",
    "}\n",
    "# Split the data into training and testing sets\n",
    "X = merged_df[features]\n",
    "y = merged_df['ELO']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a dictionary to store the models\n",
    "models = {}\n",
    "\n",
    "# Generate all combinations of 2 to all features\n",
    "combinations = []\n",
    "for r in range(2, len(features) + 1):\n",
    "    combinations.extend(itertools.combinations(features, r))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Save the MSE, Pearson correlation, and p-value in the dictionary\n",
    "\n",
    "# Perform regression for each combination and degree\n",
    "algorithms = ['linear regression', 'random forest', 'gradientBoosting', 'xgboost', 'catboost', 'TabPFN']\n",
    "\n",
    "for algo in algorithms:\n",
    "    print(f\"Running {algo} regression...\")\n",
    "    for i, combo in enumerate(combinations):\n",
    "        mse_scores = []\n",
    "        degrees = range(1, 6)\n",
    "        for degree in degrees:\n",
    "            poly = PolynomialFeatures(degree)\n",
    "            X_poly_train = poly.fit_transform(X_train[list(combo)])\n",
    "            X_poly_test = poly.transform(X_test[list(combo)])\n",
    "\n",
    "            if algo == 'linear regression':\n",
    "                model = LinearRegression()\n",
    "            elif algo == 'random forest':\n",
    "                model = RandomForestRegressor(random_state=42)\n",
    "            elif algo == 'gradientBoosting':\n",
    "                model = GradientBoostingRegressor(random_state=42)\n",
    "            elif algo == 'xgboost':\n",
    "                model = xgb.XGBRegressor(random_state=42, verbosity=0)\n",
    "            elif algo == 'catboost':\n",
    "                model = CatBoostRegressor(random_state=42, verbose=0)\n",
    "            elif algo == 'TabPFN':\n",
    "                model = TabPFNRegressor()\n",
    "\n",
    "            model.fit(X_poly_train, y_train)\n",
    "            mse = -cross_val_score(model, X_poly_test, y_test, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "            mse_scores.append(mse)\n",
    "\n",
    "        best_mse = min(mse_scores)\n",
    "        best_degree = list(degrees)[mse_scores.index(best_mse)]\n",
    "\n",
    "        if algo == 'linear regression':\n",
    "            best_model = make_pipeline(PolynomialFeatures(best_degree), LinearRegression())\n",
    "        elif algo == 'random forest':\n",
    "            best_model = make_pipeline(PolynomialFeatures(best_degree), RandomForestRegressor(random_state=42))\n",
    "        elif algo == 'gradientBoosting':\n",
    "            best_model = make_pipeline(PolynomialFeatures(best_degree), GradientBoostingRegressor(random_state=42))\n",
    "        elif algo == 'xgboost':\n",
    "            best_model = make_pipeline(PolynomialFeatures(best_degree), xgb.XGBRegressor(random_state=42, verbosity=0))\n",
    "        elif algo == 'catboost':\n",
    "            best_model = make_pipeline(PolynomialFeatures(best_degree), CatBoostRegressor(random_state=42, verbose=0))\n",
    "        elif algo == 'TabPFN':\n",
    "            best_model = make_pipeline(PolynomialFeatures(best_degree), TabPFNRegressor())\n",
    "\n",
    "        best_model.fit(X_train[list(combo)], y_train)\n",
    "        combo_str = \" \".join([feature_mapping[feat] for feat in combo])\n",
    "        key = f\"{algo}_{combo}_degree_{best_degree}\"\n",
    "        models[key] = best_model\n",
    "        joblib.dump(best_model, f\"best_model_{algo}_{combo_str}_degree_{best_degree}.pkl\")\n",
    "\n",
    "        y_pred = best_model.predict(X_test[list(combo)])\n",
    "        test_mse = mean_squared_error(y_test, y_pred)\n",
    "        pearson_corr, p_value = pearsonr(y_test, y_pred)\n",
    "        print(f\"{algo} | {combo_str} | Degree {best_degree} | Test MSE: {test_mse:.2e} | Pearson r: {pearson_corr:.2f} | p-value: {p_value:.2e}\")\n",
    "\n",
    "        results[f\"{algo}_{combo_str}\"] = {\n",
    "            'best_degree': best_degree,\n",
    "            'test_mse': test_mse,\n",
    "            'pearson_corr': pearson_corr,\n",
    "            'p_value': p_value\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d083aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ega_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
